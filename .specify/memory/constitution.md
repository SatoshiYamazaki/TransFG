<!--
Sync Impact Report
- Version change: 1.13.0 -> 1.14.0
- Modified principles: Reproducible Experiments; Test-Driven Development & Tiny Validation; Runtime Reliability & Minimal Testing; Operational Constraints; Development Workflow
- Added sections: none
- Removed sections: none
- Templates requiring updates: ✅ .specify/templates/plan-template.md; ✅ .specify/templates/spec-template.md; ✅ .specify/templates/tasks-template.md
- Follow-up TODOs: none
-->
# TransFG Constitution

## Core Principles

### Reproducible Experiments (NON-NEGOTIABLE)
Every experiment MUST be reproducible from repository state plus a committed config in models/configs.py. Training commands with all flags, random seeds, dataset splits, and output checkpoints MUST be recorded alongside results. Changes that affect determinism (augmentation, data shuffling, mixed precision) MUST note their impact and seed handling in the plan/spec. Runs MUST include a system/env stamp stored as JSON with this schema (keys, types): run_id (string), config_hash (string, SHA256), git_commit (string), os_version (string), python_version (string), torch_version (string), torchvision_version (string), ml_collections_version (string), driver_version (string), cuda_version (string), cudnn_version (string), gpu_name (string), gpu_count (int), fp16 (bool, set false when using fp32-only), seed (int), env_hash (string), tiny_train_subset_path (string), tiny_infer_subset_path (string). Config_hash MUST be SHA256 of resolved config contents (after interpolation). Env stamp location MUST be declared in the spec, linked in PRs, and PRs MUST include a short excerpt (e.g., run_id/config_hash/python_version). Recording requirements checksums is optional.

### Test-Driven Development & Tiny Validation
Test Driven Development is mandatory for new features: tests MUST be added or updated before implementation and MUST be runnable via pytest. Each feature MUST define a tiny training subset and a tiny inference subset (e.g., 2 batches per split) with maximum size ≤2 batches/split, representative label coverage (at least 2 classes when available), stable paths, and checksums for the fixture files listed in plan/spec. Provide runnable smoke commands for both train and inference (as pytest invocations or pytest-marked smoke targets) and expected quick-run timing. No feature merges without green tiny train/inference smoke runs or an explicitly approved waiver with rationale.

### Data Provenance & Licensing
Only publicly listed datasets in README (CUB-200-2011, Stanford Cars, Stanford Dogs, NABirds, iNaturalist2017) MAY be used by default. Dataset source, license terms, and any filtering or relabeling MUST be documented before training. Private or user-supplied data MUST NOT be committed; paths must stay configurable and out of version control. Any new or synthetic dataset requires maintainer approval, license review, and an explicit provenance note covering: license type, redistribution terms, attribution obligations, and data sensitivity assessment before use.

### Configuration & Change Isolation
Hyperparameters and architecture tweaks MUST live in configs (ml_collections) with safe defaults. Each change set MUST isolate one behavioral change per PR (e.g., new augment policy, optimizer setting) and include rationale plus expected effect. Hidden defaults in scripts are prohibited; every runtime knob must be discoverable in config or CLI flags.

### Variant Traceability & Fair Comparisons
Backbone choice (e.g., ViT-B/L/H from CLIP, SigLIP2, DINOv2), part attention toggles, and patch/resolution settings MUST be declared in configs with clear names. Comparisons MUST hold preprocessing, dataset splits, and evaluation metrics constant; when they differ, the impact MUST be justified and logged. Results MUST reference the exact pretrained weights (source/commit) and normalization used per backbone. Normalization identifiers (mean/std set or transform name) MUST be logged with the run and surfaced in artifacts and config names.

### Baseline Fidelity (Linear Probe)
For each backbone family (CLIP, SigLIP2, DINOv2), a frozen-encoder linear-probe baseline MUST be run and reported before or alongside finetuning. Commands, configs, feature-extraction resolution, and normalization must be recorded. Finetune claims MUST cite the paired linear-probe result for the same backbone and preprocessing. Baseline metrics MUST be stored as a tabular artifact (CSV/JSON) containing backbone, config name, normalization ID, resolution, head size, seed, checkpoint/hash, top-1/other metrics, per-class metrics (e.g., macro-F1 or per-class accuracy), and linked finetune run_id/checkpoint_id.

### Experiment Logging & Visibility
All training runs MUST emit TensorBoard logs (loss/accuracy curves and key scalars) to a committed-relative path. Inference outputs generated during training (e.g., val/test predictions) MUST be persisted in a format consumable by FiftyOne, including labels, logits/scores, and metadata for backbone, config, and checkpoint. Log and artifact paths MUST follow a stable, documented schema recorded in plan/spec/PRs; hashed path hierarchies are not required. Minimum contents: per-class accuracy, confusion matrix, top-k metrics, and a misclassification view in FiftyOne. Retention/TTL: keep TensorBoard logs, FiftyOne artifacts, and baseline tables for at least 90 days (or longer if tied to a release) and state retention evidence (TTL policy link or storage path with expiry note) in PRs. Checkpoints MUST NOT be committed; store them in external or ignored locations referenced by checkpoint_id and record a SHA256 of the checkpoint file in run metadata.

### Evaluation Fidelity
Evaluation MUST use the canonical splits and image resolutions for each dataset. Report at minimum top-1 accuracy and loss; note evaluation batch size, checkpoint used, and any test-time augmentation. Benchmarks MUST cite the reference paper/commit they compare against and avoid mixing incompatible preprocessing or label spaces. Label mapping/ordering MUST be validated against dataset definitions whenever preprocessing, augmentations, or backbones change to prevent silent misalignment. Save a label-map artifact (JSON) at the documented run output path and record its checksum when available; per-class metrics (e.g., per-class accuracy) MUST be logged in TensorBoard/FiftyOne. A minimal evaluation sanity check (single-batch eval) is mandatory unless explicitly waived with rationale.

### Runtime Reliability & Minimal Testing
Before long runs, smoke tests MUST execute the tiny training subset and tiny inference subset to validate data loading, model construction, forward/backward passes (FP16 if used), and basic evaluation plumbing. Smoke tests SHOULD be implemented as pytest tests (e.g., pytest -m smoke) exercising the tiny subsets. Training runs MUST specify GPU/MPS count, distributed launch parameters (if any), and resource bounds to avoid contention. Failures MUST capture logs with the command and config used. System info (GPU/MPS model/count, CUDA/cuDNN or MPS driver) MUST be captured once per run and stored with logs. The evaluation sanity check requirement lives in Evaluation Fidelity; adhere to it unless a waiver is recorded with rationale. Tiny subset paths and checksums MUST remain stable and be cited in run metadata.

## Operational Constraints
Target stack (modern): Python 3.10+ and PyTorch 2.x with matching torchvision. Primary development environment is MacBook Air M2 (arm64); commands and dependencies MUST be runnable on macOS with CPU/MPS (no CUDA assumed). Legacy stack (Python 3.7.x, PyTorch 1.5.1, torchvision 0.6.1) is supported only until 2026-06-30; migrations MUST target the modern stack before that date. After 2026-06-30, new PRs MUST target the modern stack or include a time-bound waiver approved by a maintainer. Training is GPU/MPS-bound and assumes torch.distributed launch only when explicitly required; single-device (CPU or MPS) paths MUST remain working. Keep requirements.txt authoritative; do not vendor weights or datasets into the repo. FP16 is optional but, if enabled, must include loss-scaling settings. Backbones may use official weights from CLIP, SigLIP2, or DINOv2; record source/commit for each and never commit checkpoints. These modern backbones MUST NOT be used until the plan/spec/PR declares the upgraded stack (Python/PyTorch/extra libs), updates README and requirements, and provides a recorded validation command/output proving TensorBoard and FiftyOne run on the declared stack; the validation command MUST be checked into the repo (e.g., make target or script) and runnable on Mac M2 (CPU/MPS).

## Development Workflow
Use feature branches per change. Every PR MUST include: (a) training/eval command(s) and config path, (b) dataset provenance note if data handling changes, (c) smoke-test evidence on the tiny training and inference subsets (pytest-run, or rationale if skipped), (d) recorded metrics/log location, and (e) tests added/updated before implementation per TDD (pytest). Documentation updates (README snippets or per-feature quickstart) MUST accompany new configs or flags. PRs MUST state log/artifact retention window and evidence (TTL policy link or storage path with expiry note) for TensorBoard, FiftyOne, and baseline tables, and MUST surface the env stamp link (path declared in spec) with an excerpt. PRs enabling CLIP/SigLIP2/DINOv2 MUST include README/requirements updates plus recorded validation command/output showing TensorBoard and FiftyOne run on the declared stack, and MUST add the validation command/target to the repo; validation MUST be runnable on Mac M2 (CPU/MPS). No merge if Constitution Check gates in plan.md/tasks.md fail. PRs opened after 2026-06-30 must target the modern stack or include a maintainer-approved, time-bound waiver.

## Governance
This constitution supersedes other process docs when in conflict. Amendments require a PR citing the change, updated version, and rationale; at least one maintainer must approve. Versioning follows semantic rules: MAJOR for breaking governance/principle removals, MINOR for added or expanded principles/sections, PATCH for clarifications. Any change to environment.yml/conda export MUST be called out in the PR, include an updated env hash and requirements.txt checksum, and requires at least a PATCH version bump. Compliance is reviewed during planning (Constitution Check), pre-merge reviews, and before releasing new checkpoints. PRs must confirm retention evidence (TTL or storage policy) for TensorBoard/FiftyOne/baseline artifacts. After 2026-06-30, Governance review MUST block merges that do not target the modern stack unless a dated waiver is recorded.

**Version**: 1.14.0 | **Ratified**: 2026-01-19 | **Last Amended**: 2026-01-19
